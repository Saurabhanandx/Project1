# -*- coding: utf-8 -*-
"""CUSTOMER CHURN PREDICTION MODEL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1e45b-iLSNvLIL6uZ0jVw7D_IM7EEDd

## DATA PREPROCESSING
"""

## LOADING THE REQUIRED LIBRARIES
import pandas as pd
from openpyxl import load_workbook

df = pd.read_excel('sample_data/customer_churn_large_dataset.xlsx') #loading the dataset

df.drop_duplicates() #removing the duplicate rows

df.dropna() #HANDLING MISSING VALUES

missing_data = df.isna().sum()

print(missing_data)

# Frequency of classes of dependent variable
df["Churn"].value_counts()

df.head(10)

# Customers leaving the bank
churn = df.loc[df["Churn"]==1]

# Customers who did not leave the bank
not_churn = df.loc[df["Churn"]==0]

# Frequency of not_churn group according to Tenure
not_churn["Subscription_Length_Months"].value_counts().sort_values()

# Frequency of churn group according to Tenure
churn["Subscription_Length_Months"].value_counts().sort_values()

# Frequency of not_churn group according to Geography
not_churn.Location.value_counts().sort_values()

# Frequency of churn group according to Geography
churn.Location.value_counts().sort_values()

# Frequency of churn group according to Gender
churn.Gender.value_counts()

# Frequency of not_churn group according to Gender
not_churn.Gender.value_counts()

"""VISUALIZING

"""

import matplotlib.pyplot as plt
from matplotlib import pyplot

# distribution of the Monthly Bill for not_churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Monthly_Bill')
pyplot.hist(not_churn["Monthly_Bill"],bins=15, alpha=0.7, label='Not Churn')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Monthly Bill for churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Monthly_Bill')
pyplot.hist(churn["Monthly_Bill"],bins=15, alpha=0.8, label='Churn')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Age for not_churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Age')
pyplot.hist(not_churn["Age"],bins=15, alpha=0.7, label='Not Churn')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Age for not_churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Age')
pyplot.hist(churn["Age"],bins=15, alpha=0.7, label='Churn')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Total_Usage_GB for not_churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Total_Usage_GB')
pyplot.hist(not_churn["Total_Usage_GB"],bins=15, alpha=0.7, label='Not Churn')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Total_Usage_GB for churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('Total_Usage_GB')
pyplot.hist(churn["Total_Usage_GB"],bins=15, alpha=0.7, label='Churn')
pyplot.legend(loc='upper right')
pyplot.show()

"""ONE HOT ENCODING"""

# Variables to apply one hot encoding
list = ["Gender", "Location"]
df = pd.get_dummies(df, columns =list, drop_first = True)

df.head()

# Removing variables that will not affect the dependent variable
df = df.drop(["CustomerID","Name"], axis = 1)

df.head(10)

"""SCALING"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import accuracy_score,recall_score
from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, GridSearchCV

X = df.drop("Churn",axis=1)
y = df["Churn"]
# Train-Test Separation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=12345)

# Train the classifier on the training data
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(f'Classification Report:\n{report}')
print(f'Confusion Matrix:\n{confusion}')

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Models for Classification
models = [('LR', LogisticRegression(random_state=123456)),
          ('KNN', KNeighborsClassifier()),
          ('CART', DecisionTreeClassifier(random_state=123456)),
           ('GB', GradientBoostingClassifier(random_state = 12345))]

results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=10)
    cv_results = cross_val_score(model, X, y, cv=kfold)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# GB Confusion Matrix
model_GB = GradientBoostingClassifier(random_state=12345)
model_GB.fit(X_train, y_train)
y_pred = model_GB.predict(X_test)
conf_mat = confusion_matrix(y_pred,y_test)
conf_mat

# Classification Report for XGB Model
print(classification_report(model_GB.predict(X_test),y_test))

# Auc Roc Curve
def generate_auc_roc_curve(clf, X_test):
    y_pred_proba = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
    plt.legend(loc=4)
    plt.show()
    pass

# LightGBM:
lgb_model = LGBMClassifier()
# Model Tuning
lgbm_params = {'colsample_bytree': 0.5,
 'learning_rate': 0.01,
 'max_depth': 6,
 'n_estimators': 500}

lgbm_tuned = LGBMClassifier(**lgbm_params).fit(X, y)

#Let's choose the highest 4 models
# GBM
gbm_model = GradientBoostingClassifier()
# Model Tuning
gbm_params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1}
gbm_tuned = GradientBoostingClassifier(**gbm_params).fit(X,y)

# evaluate each model in turn
models = [("LightGBM", lgbm_tuned),
          ("GB",gbm_tuned)]
results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=10)
    cv_results = cross_val_score(model, X, y, cv=10, scoring="accuracy")
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

import seaborn as sns

for name, model in models:
        base = model.fit(X_train,y_train)
        y_pred = base.predict(X_test)
        acc_score = accuracy_score(y_test, y_pred)
        feature_imp = pd.Series(base.feature_importances_,
                        index=X.columns).sort_values(ascending=False)

        sns.barplot(x=feature_imp, y=feature_imp.index)
        plt.xlabel('X axis')
        plt.ylabel('Y axis')
        plt.title(name)
        plt.show()

